{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a106713f",
   "metadata": {},
   "source": [
    "## Store GBIF Occurrence Data Set locally\n",
    "\n",
    "For Virtual Herbarium Germany (BGBM) see <https://doi.org/10.15468/dl.tued2e>. We saved all of it into the local data directory `data/VHde_0195853-230224095556074_BGBM/`:\n",
    "- We need `occurrence.txt` as basic data file, which is 1GB large—please download it first (**it will not be in the official GitHub documentation**) or change the code here to read your special input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7530a19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, GBIF data found. Results will later be written to data/VHde_0195853-230224095556074_BGBM/occurrence_recordedBy_occurrenceIDs_20230703.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "gbif_dataset_path=\"data/VHde_0195853-230224095556074_BGBM\"\n",
    "# join file name for saving results\n",
    "this_output_tabdata_file=os.path.join(\n",
    "    gbif_dataset_path, (\"occurrence_recordedBy_occurrenceIDs_%s.tsv\" % datetime.today().strftime('%Y%m%d'))\n",
    ")\n",
    "\n",
    "if not os.path.exists(gbif_dataset_path):\n",
    "    print(\"Where is the folder of are GBIF occurrence data?\", gbif_dataset_path, \"not found\")\n",
    "    print(\"Recommendation is use a subfolder, e.g. “data/VHde_0195853-230224095556074_BGBM”\")\n",
    "else:\n",
    "    print(\"OK, GBIF data found. Results will later be written to\", this_output_tabdata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93e228",
   "metadata": {},
   "source": [
    "## Read GBIF Occurrence Data\n",
    "\n",
    "Get `recordedBy` of `occurrence.txt` and look into the data first, data columns aso. …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db47f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gbifID', 'abstract', 'accessRights', 'accrualMethod', 'accrualPeriodicity', 'accrualPolicy', 'alternative', 'audience', 'available', 'bibliographicCitation', 'conformsTo', 'contributor', 'coverage', 'created', 'creator', 'date', 'dateAccepted', 'dateCopyrighted', 'dateSubmitted', 'description', 'educationLevel', 'extent', 'format', 'hasFormat', 'hasPart', 'hasVersion', 'identifier', 'instructionalMethod', 'isFormatOf', 'isPartOf', 'isReferencedBy', 'isReplacedBy', 'isRequiredBy', 'isVersionOf', 'issued', 'language', 'license', 'mediator', 'medium', 'modified', 'provenance', 'publisher', 'references', 'relation', 'replaces', 'requires', 'rights', 'rightsHolder', 'source', 'spatial', 'subject', 'tableOfContents', 'temporal', 'title', 'type', 'valid', 'institutionID', 'collectionID', 'datasetID', 'institutionCode', 'collectionCode', 'datasetName', 'ownerInstitutionCode', 'basisOfRecord', 'informationWithheld', 'dataGeneralizations', 'dynamicProperties', 'occurrenceID', 'catalogNumber', 'recordNumber', 'recordedBy', 'recordedByID', 'individualCount', 'organismQuantity', 'organismQuantityType', 'sex', 'lifeStage', 'reproductiveCondition', 'behavior', 'establishmentMeans', 'degreeOfEstablishment', 'pathway', 'georeferenceVerificationStatus', 'occurrenceStatus', 'preparations', 'disposition', 'associatedOccurrences', 'associatedReferences', 'associatedSequences', 'associatedTaxa', 'otherCatalogNumbers', 'occurrenceRemarks', 'organismID', 'organismName', 'organismScope', 'associatedOrganisms', 'previousIdentifications', 'organismRemarks', 'materialSampleID', 'eventID', 'parentEventID', 'fieldNumber', 'eventDate', 'eventTime', 'startDayOfYear', 'endDayOfYear', 'year', 'month', 'day', 'verbatimEventDate', 'habitat', 'samplingProtocol', 'sampleSizeValue', 'sampleSizeUnit', 'samplingEffort', 'fieldNotes', 'eventRemarks', 'locationID', 'higherGeographyID', 'higherGeography', 'continent', 'waterBody', 'islandGroup', 'island', 'countryCode', 'stateProvince', 'county', 'municipality', 'locality', 'verbatimLocality', 'verbatimElevation', 'verticalDatum', 'verbatimDepth', 'minimumDistanceAboveSurfaceInMeters', 'maximumDistanceAboveSurfaceInMeters', 'locationAccordingTo', 'locationRemarks', 'decimalLatitude', 'decimalLongitude', 'coordinateUncertaintyInMeters', 'coordinatePrecision', 'pointRadiusSpatialFit', 'verbatimCoordinateSystem', 'verbatimSRS', 'footprintWKT', 'footprintSRS', 'footprintSpatialFit', 'georeferencedBy', 'georeferencedDate', 'georeferenceProtocol', 'georeferenceSources', 'georeferenceRemarks', 'geologicalContextID', 'earliestEonOrLowestEonothem', 'latestEonOrHighestEonothem', 'earliestEraOrLowestErathem', 'latestEraOrHighestErathem', 'earliestPeriodOrLowestSystem', 'latestPeriodOrHighestSystem', 'earliestEpochOrLowestSeries', 'latestEpochOrHighestSeries', 'earliestAgeOrLowestStage', 'latestAgeOrHighestStage', 'lowestBiostratigraphicZone', 'highestBiostratigraphicZone', 'lithostratigraphicTerms', 'group', 'formation', 'member', 'bed', 'identificationID', 'verbatimIdentification', 'identificationQualifier', 'typeStatus', 'identifiedBy', 'identifiedByID', 'dateIdentified', 'identificationReferences', 'identificationVerificationStatus', 'identificationRemarks', 'taxonID', 'scientificNameID', 'acceptedNameUsageID', 'parentNameUsageID', 'originalNameUsageID', 'nameAccordingToID', 'namePublishedInID', 'taxonConceptID', 'scientificName', 'acceptedNameUsage', 'parentNameUsage', 'originalNameUsage', 'nameAccordingTo', 'namePublishedIn', 'namePublishedInYear', 'higherClassification', 'kingdom', 'phylum', 'class', 'order', 'family', 'subfamily', 'genus', 'genericName', 'subgenus', 'infragenericEpithet', 'specificEpithet', 'infraspecificEpithet', 'cultivarEpithet', 'taxonRank', 'verbatimTaxonRank', 'vernacularName', 'nomenclaturalCode', 'taxonomicStatus', 'nomenclaturalStatus', 'taxonRemarks', 'datasetKey', 'publishingCountry', 'lastInterpreted', 'elevation', 'elevationAccuracy', 'depth', 'depthAccuracy', 'distanceAboveSurface', 'distanceAboveSurfaceAccuracy', 'distanceFromCentroidInMeters', 'issue', 'mediaType', 'hasCoordinate', 'hasGeospatialIssues', 'taxonKey', 'acceptedTaxonKey', 'kingdomKey', 'phylumKey', 'classKey', 'orderKey', 'familyKey', 'genusKey', 'subgenusKey', 'speciesKey', 'species', 'acceptedScientificName', 'verbatimScientificName', 'typifiedName', 'protocol', 'lastParsed', 'lastCrawled', 'repatriated', 'relativeOrganismQuantity', 'level0Gid', 'level0Name', 'level1Gid', 'level1Name', 'level2Gid', 'level2Name', 'level3Gid', 'level3Name', 'iucnRedListCategory', 'eventType']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # to read data\n",
    "\n",
    "# Reading all at once does not work to read 1GB of data yet\n",
    "occurrences = pd.read_csv(\n",
    "    os.path.join(gbif_dataset_path, \"occurrence.txt\"), sep=\"\\t\", low_memory=False,\n",
    "    nrows=1\n",
    ")\n",
    "\n",
    "print(list(occurrences.columns)) # 259 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d3a133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occurrenceID</th>\n",
       "      <th>recordedBy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Kurt Harz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Eugen Erdner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Alois Zick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>J. Kraenzle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hermann Poeverlein</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   occurrenceID          recordedBy\n",
       "0           NaN           Kurt Harz\n",
       "1           NaN        Eugen Erdner\n",
       "2           NaN          Alois Zick\n",
       "3           NaN         J. Kraenzle\n",
       "4           NaN  Hermann Poeverlein"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just see the first rows\n",
    "occurrences = pd.read_csv(\n",
    "    os.path.join(gbif_dataset_path, \"occurrence.txt\"), sep=\"\\t\", low_memory=False,\n",
    "    usecols=[\"occurrenceID\", \"recordedBy\"],\n",
    "    nrows=50 # read all data results in memory kill\n",
    ")\n",
    "occurrences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b82390",
   "metadata": {},
   "source": [
    "We follow <https://towardsdatascience.com/tips-and-tricks-for-loading-large-csv-files-into-pandas-dataframes-part-2-5fc02fc4e3ab> and filter for having an occourrenceID.\n",
    "\n",
    "For large data sets it is better to read it defining a “chunksize” (because otherwise the processor would read all at once and gets stuck):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67e514a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read large data as chunk 0.014245271682739258 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "chunks_occurrences = pd.read_csv(\n",
    "    os.path.join(gbif_dataset_path, \"occurrence.txt\"), sep=\"\\t\", low_memory=False,\n",
    "    usecols=[\"occurrenceID\", \"recordedBy\"],\n",
    "    chunksize=100000\n",
    ")\n",
    "\n",
    "print(\"read large data as chunk\", time.time() - starttime, 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d63f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter having occurrenceID: (75997, 2)\n",
      "filter having occurrenceID: (100000, 2)\n",
      "filter having occurrenceID: (5567, 2)\n",
      "filter having occurrenceID: (32993, 2)\n",
      "filter having occurrenceID: (99970, 2)\n",
      "filter having occurrenceID: (72362, 2)\n",
      "filter having occurrenceID: (100000, 2)\n",
      "filter having occurrenceID: (100000, 2)\n",
      "filter having occurrenceID: (27751, 2)\n",
      "filter having occurrenceID: (0, 2)\n",
      "filter having occurrenceID: (0, 2)\n",
      "process data having only occurrenceID took 16.236688375473022  seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occurrenceID</th>\n",
       "      <th>recordedBy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18004</th>\n",
       "      <td>https://je.jacq.org/JE00003434</td>\n",
       "      <td>Ecklon,C.F. &amp; Zeyher,C.L.P.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18005</th>\n",
       "      <td>https://je.jacq.org/JE00003433</td>\n",
       "      <td>Ecklon,C.F. &amp; Zeyher,C.L.P.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18006</th>\n",
       "      <td>https://je.jacq.org/JE00003435</td>\n",
       "      <td>Ecklon,C.F. &amp; Zeyher,C.L.P.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18007</th>\n",
       "      <td>https://je.jacq.org/JE00003436</td>\n",
       "      <td>Ecklon,C.F. &amp; Zeyher,C.L.P.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18008</th>\n",
       "      <td>https://je.jacq.org/JE00003430</td>\n",
       "      <td>Zenker,G.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         occurrenceID                   recordedBy\n",
       "18004  https://je.jacq.org/JE00003434  Ecklon,C.F. & Zeyher,C.L.P.\n",
       "18005  https://je.jacq.org/JE00003433  Ecklon,C.F. & Zeyher,C.L.P.\n",
       "18006  https://je.jacq.org/JE00003435  Ecklon,C.F. & Zeyher,C.L.P.\n",
       "18007  https://je.jacq.org/JE00003436  Ecklon,C.F. & Zeyher,C.L.P.\n",
       "18008  https://je.jacq.org/JE00003430                    Zenker,G."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_having_occurrenceID(df):\n",
    "    df = df[df.occurrenceID.notnull()]\n",
    "    print(\"filter having occurrenceID: \" + str(df.shape))\n",
    "    # print(df.shape)\n",
    "    return df\n",
    "\n",
    "starttime = time.time()\n",
    "chunk_list = [] # used for storing dataframes\n",
    "for chunk in chunks_occurrences:\n",
    "    # each chunk is a dataframe\n",
    "    # perform data filtering\n",
    "    filtered_chunk = filter_having_occurrenceID(chunk)\n",
    "    # Once the data filtering is done, append the filtered chunk to list\n",
    "    chunk_list.append(filtered_chunk)\n",
    "\n",
    "# concat all the dfs in the list in\n",
    "occurrences = pd.concat(chunk_list)\n",
    "\n",
    "print(\"process data having only occurrenceID took\", time.time() - starttime, ' seconds')\n",
    "occurrences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2790fe65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recordedBy</th>\n",
       "      <th>occurrenceID_count</th>\n",
       "      <th>occurrenceID_firstsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52725</th>\n",
       "      <td>Żelazny,J.</td>\n",
       "      <td>4</td>\n",
       "      <td>https://herbarium.bgbm.org/object/B100344466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52726</th>\n",
       "      <td>Ždanova,O.</td>\n",
       "      <td>5</td>\n",
       "      <td>https://herbarium.bgbm.org/object/B100263330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52727</th>\n",
       "      <td>Žíla,V.</td>\n",
       "      <td>3</td>\n",
       "      <td>https://herbarium.bgbm.org/object/B100009590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52728</th>\n",
       "      <td>Волкова Е.</td>\n",
       "      <td>1</td>\n",
       "      <td>https://herbarium.bgbm.org/object/B100530714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52729</th>\n",
       "      <td>Жирова,O.</td>\n",
       "      <td>1</td>\n",
       "      <td>https://herbarium.bgbm.org/object/B100630811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       recordedBy  occurrenceID_count  \\\n",
       "52725  Żelazny,J.                   4   \n",
       "52726  Ždanova,O.                   5   \n",
       "52727     Žíla,V.                   3   \n",
       "52728  Волкова Е.                   1   \n",
       "52729   Жирова,O.                   1   \n",
       "\n",
       "                           occurrenceID_firstsample  \n",
       "52725  https://herbarium.bgbm.org/object/B100344466  \n",
       "52726  https://herbarium.bgbm.org/object/B100263330  \n",
       "52727  https://herbarium.bgbm.org/object/B100009590  \n",
       "52728  https://herbarium.bgbm.org/object/B100530714  \n",
       "52729  https://herbarium.bgbm.org/object/B100630811  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group and aggregate data: \n",
    "occurrences_unique=occurrences.groupby(['recordedBy']).agg(\n",
    "    occurrenceID_count= ('occurrenceID', 'count'), # use count function\n",
    "    occurrenceID_firstsample=('occurrenceID', lambda x: list(x)[0]) # custom function, to get the first entry\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "occurrences_unique.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f999e270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write these tabbed data into data/VHde_0195853-230224095556074_BGBM/occurrence_recordedBy_occurrenceIDs_20230703.tsv\n"
     ]
    }
   ],
   "source": [
    "print(\"Write these tabbed data into\", this_output_tabdata_file)\n",
    "\n",
    "occurrences_unique.to_csv(this_output_tabdata_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c0f0c",
   "metadata": {},
   "source": [
    "## Parsing with dwcagent_bin\n",
    "\n",
    "Dependency Ruby Gem package <https://libraries.io/rubygems/dwc_agent> has to be installed and Ruby itself.\n",
    "\n",
    "You can use the ruby script in `bin/agent_parse4tsv.rb` and change the code inside for file input and output. After that you can run it like:\n",
    "```bash\n",
    "cd bin/agent_parse4tsv.rb\n",
    "ruby agent_parse4tsv.rb\n",
    "\n",
    "# or if you want to measure how fast it parses use\n",
    "time ruby agent_parse4tsv.rb\n",
    "# real    0m41,923s\n",
    "# user    0m23,390s\n",
    "# sys     0m16,252s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fcbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
